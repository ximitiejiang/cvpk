#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Nov 20 10:13:25 2018

@author: suliang
"""
import numpy as np
import torch
from torch import nn

def batch_normalization():
    '''batch_normalization的意义在于：
    第一步：减均值，除方差。这一步用于归一化，把数据移动到均值为0方差为1的区域。
    这样的好处是该区域的激活函数对应的导数较大，不容易产生梯度消失，从而网络无法学习或训练缓慢
    
    第二步：乘gama，加belta。这一步增加两个可学习参数，目的是避免归一化到(0,1)之后反而效果不好

    之所以在(0,1)反而效果不好，是因为有的数据本身就不是对称在(0,1)的，或者激活函数在(0,1)的导数
    变化不大，或者本来就需要不对称性的或者在饱和区的数据分布。
    所以相当于逆归一化操作恢复到一个待学习的均值方差位置，这样的参数学习的方式，最终输出的数据分布
    可能是(0,1)分布，也可能是别的任何一种分布，取决于学习的结果。
    最好的情况是拉回(0,1)分布，不容易产生梯度消失，最坏的情况是通过学习恢复回到原始分布
    
    Bn务必在做非线性变换之前完成，也就是在激活函数之前（这在原作论文提到）
    
    同时，最好对整个训练集计算均值和方差，用来作为最终使用的均值方差。
    参考：《深度学习与计算机视觉》 p141
    '''
    
    def sigmoid(x):
        return x*1
    
    x = torch.tensor([i for i in range(-20.,-10.,1)])
    output = nn.BatchNorm1d(x)
    
    

    